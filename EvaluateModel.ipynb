{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "from actinet.models import ActivityClassifier\n",
    "from actinet.prepare import load_all_and_make_windows, extract_accelerometer_features, \\\n",
    "    prepare_accelerometer_data\n",
    "from actinet.evaluate import evaluate_preprocessing, evaluate_models\n",
    "\n",
    "WINSEC = 30 # seconds\n",
    "SAMPLE_RATE = 100 # Hz\n",
    "RESAMPLE_RATE = 30 # Hz\n",
    "N_JOBS = 8 # Set to higher number for quicker execution, but don't exceed max.\n",
    "\n",
    "DATAFILES = f\"data/capture24/P[0-9][0-9][0-9].csv.gz\"\n",
    "ANNOFILE = f\"data/capture24/annotation-label-dictionary.csv\"\n",
    "SAVEFOLDER = f\"data/capture24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalate pre-processing steps\n",
    "\n",
    "The purpose of this section is to evaluate the model performance compare to different preprocessing approaches, as well as model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1 - Actipy downsampling, no lp filter\n",
    "\n",
    "DATAFILES = f\"data/capture24/P[0-9][0-9][0-9].csv.gz\"\n",
    "ANNOFILE = f\"data/capture24/annotation-label-dictionary.csv\"\n",
    "SAVEFOLDER = f\"data/capture24\"\n",
    "\n",
    "if len(glob(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_None/*.npy\")) == 4:\n",
    "    X_nn = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_None/X.npy\")\n",
    "    Y_nn = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_None/Y.npy\")\n",
    "    T_nn = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_None/T.npy\")\n",
    "    pid_nn = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_None/pid.npy\")\n",
    "\n",
    "else:\n",
    "    X_nn, Y_nn, T_nn, pid_nn = load_all_and_make_windows(\n",
    "        datafiles=glob(DATAFILES), \n",
    "        annofile=ANNOFILE, \n",
    "        out_dir=SAVEFOLDER, \n",
    "        anno_label=\"Walmsley2020\",\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        winsec=WINSEC,\n",
    "        n_jobs=N_JOBS,\n",
    "        downsampling_method=\"nn\",\n",
    "        lowpass_hz=None,\n",
    "        resample_rate=RESAMPLE_RATE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 - Actipy downsampling, 15Hz lp filter\n",
    "\n",
    "DATAFILES = f\"data/capture24/P[0-9][0-9][0-9].csv.gz\"\n",
    "ANNOFILE = f\"data/capture24/annotation-label-dictionary.csv\"\n",
    "SAVEFOLDER = f\"data/capture24\"\n",
    "\n",
    "if len(glob(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_15/*.npy\")) == 4:\n",
    "    X_nn15 = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_15/X.npy\")\n",
    "    Y_nn15 = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_15/Y.npy\")\n",
    "    T_nn15 = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_15/T.npy\")\n",
    "    pid_nn15 = np.load(f\"{SAVEFOLDER}/prepared/downsampling_nn_lowpass_15/pid.npy\")\n",
    "\n",
    "else:\n",
    "    X_nn15, Y_nn15, T_nn15, pid_nn15 = load_all_and_make_windows(\n",
    "        datafiles=glob(DATAFILES), \n",
    "        annofile=ANNOFILE, \n",
    "        out_dir=SAVEFOLDER, \n",
    "        anno_label=\"Walmsley2020\",\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        winsec=WINSEC,\n",
    "        n_jobs=N_JOBS,\n",
    "        downsampling_method=\"nn\",\n",
    "        lowpass_hz=15,\n",
    "        resample_rate=RESAMPLE_RATE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3 - Linear downsampling, no lp filter\n",
    "\n",
    "DATAFILES = f\"data/capture24/P[0-9][0-9][0-9].csv.gz\"\n",
    "ANNOFILE = f\"data/capture24/annotation-label-dictionary.csv\"\n",
    "SAVEFOLDER = f\"data/capture24\"\n",
    "\n",
    "if len(glob(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/*.npy\")) == 4:\n",
    "    X_linear = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/X.npy\")\n",
    "    Y_linear = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/Y.npy\")\n",
    "    T_linear = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/T.npy\")\n",
    "    pid_linear = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/pid.npy\")\n",
    "\n",
    "else:\n",
    "    X_linear, Y_linear, T_linear, pid_linear = load_all_and_make_windows(\n",
    "        datafiles=glob(DATAFILES), \n",
    "        annofile=ANNOFILE, \n",
    "        out_dir=SAVEFOLDER, \n",
    "        anno_label=\"Walmsley2020\",\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        winsec=WINSEC,\n",
    "        n_jobs=N_JOBS,\n",
    "        downsampling_method=\"linear\",\n",
    "        lowpass_hz=None,\n",
    "        resample_rate=RESAMPLE_RATE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"models/evaluation_models\"\n",
    "\n",
    "def reset_folder(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path)\n",
    "\n",
    "reset_folder(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_nn = ActivityClassifier(\n",
    "    labels = np.unique(Y_nn),\n",
    "    batch_size=1000,\n",
    "    device=\"cuda:0\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "Y_pred_nn = evaluate_preprocessing(classifier_nn, X_nn, Y_nn, pid_nn, T_nn, \n",
    "                     f\"{models_path}/downsampling_nn_lowpass_None_{{}}.pt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_nn15 = ActivityClassifier(\n",
    "    labels = np.unique(Y_nn),\n",
    "    batch_size=1000,\n",
    "    device=\"cuda:0\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "Y_pred_nn15 = evaluate_preprocessing(classifier_nn15, X_nn15, Y_nn15, pid_nn15, T_nn15, \n",
    "                       f\"{models_path}/downsampling_nn_lowpass_15_{{}}.pt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_linear = ActivityClassifier(\n",
    "    labels = np.unique(Y_nn),\n",
    "    batch_size=1000,\n",
    "    device=\"cuda:0\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "Y_pred_linear = evaluate_preprocessing(classifier_linear, X_linear, Y_linear, pid_linear, T_linear,\n",
    "                         f\"{models_path}/downsampling_linear_lowpass_None_{{}}.pt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'nearest neighbour no filter': {'y': Y_nn, 'y_pred': Y_pred_nn, 'pid': pid_nn},\n",
    "    'nearest neighbour 15Hz lp filter': {'y': Y_nn15, 'y_pred': Y_pred_nn15, 'pid': pid_nn15},\n",
    "    'linear downsampling no filter': {'y': Y_linear, 'y_pred': Y_pred_linear, 'pid': pid_linear}\n",
    "}\n",
    "\n",
    "# Define a function to calculate evaluation metrics for each participant\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    return accuracy, f1, kappa\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = []\n",
    "\n",
    "# Calculate metrics for each model and participant\n",
    "for model, model_data in data.items():\n",
    "    for pid in np.unique(model_data['pid']):\n",
    "        mask = model_data['pid'] == pid\n",
    "        y_true = model_data['y'][mask]\n",
    "        y_pred = model_data['y_pred'][mask]\n",
    "        accuracy, f1, kappa = calculate_metrics(y_true, y_pred)\n",
    "        results.append({'Participant': pid, 'Model': model, \n",
    "                        'Accuracy': accuracy, 'Macro F1': f1, 'Cohen Kappa': kappa})\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Aggregate results by participant\n",
    "agg_results = results.groupby('Model').agg({'Accuracy': ['mean', 'std'],\n",
    "                                                  'Macro F1': ['mean', 'std'],\n",
    "                                                  'Cohen Kappa': ['mean', 'std']})\n",
    "\n",
    "# Rename columns for clarity\n",
    "agg_results.columns = ['Accuracy Mean', 'Accuracy Std', 'Macro F1 Mean', 'Macro F1 Std', 'Cohen Kappa Mean', 'Cohen Kappa Std']\n",
    "\n",
    "\n",
    "def format_mean_std(mean, std):\n",
    "    return f\"{mean:.3f} \\u00B1 {std:.3f}\"\n",
    "\n",
    "agg_results[\"Accuracy\"] = agg_results.apply(lambda x: format_mean_std(x[\"Accuracy Mean\"], \n",
    "                                                                      x[\"Accuracy Std\"]), axis=1)\n",
    "\n",
    "agg_results[\"Macro F1\"] = agg_results.apply(lambda x: format_mean_std(x[\"Macro F1 Mean\"], \n",
    "                                                                      x[\"Macro F1 Std\"]), axis=1)\n",
    "\n",
    "agg_results[\"Cohen Kappa\"] = agg_results.apply(lambda x: format_mean_std(x[\"Cohen Kappa Mean\"], \n",
    "                                                                      x[\"Cohen Kappa Std\"]), axis=1)\n",
    "\n",
    "agg_results[[\"Accuracy\", \"Macro F1\", \"Cohen Kappa\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for each model\n",
    "for model, model_data in data.items():\n",
    "    y_true = model_data['y']\n",
    "    y_pred = model_data['y_pred']\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')  # Normalized confusion matrix\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.3f', cbar=False)\n",
    "    plt.title(f'Confusion Matrix - {model}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    plt.xticks(ticks=np.arange(len(cm))+0.5, labels=np.unique(y_true))\n",
    "    plt.yticks(ticks=np.arange(len(cm))+0.5, labels=np.unique(y_true))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate actinet against accelerometer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract the features each of the capture 24 files using the accelerometer package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_accelerometer_features(n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare the participant accelerometer data into the expected shape, containing the X,Y,T and P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerometer feature data prepared\n",
    "if len(glob(f\"{SAVEFOLDER}/prepared/accelerometer/*.npy\")) == 4:\n",
    "    X_bbaa = np.load(f\"{SAVEFOLDER}/prepared/accelerometer/X.npy\")\n",
    "    Y_bbaa = np.load(f\"{SAVEFOLDER}/prepared/accelerometer/Y.npy\")\n",
    "    T_bbaa = np.load(f\"{SAVEFOLDER}/prepared/accelerometer/T.npy\")\n",
    "    P_bbaa = np.load(f\"{SAVEFOLDER}/prepared/accelerometer/pid.npy\")\n",
    "\n",
    "else:\n",
    "    X_bbaa, Y_bbaa, T_bbaa, P_bbaa = prepare_accelerometer_data(ANNOFILE, SAVEFOLDER, N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actinet data prepared\n",
    "if len(glob(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/*.npy\")) == 4:\n",
    "    X_actinet = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/X.npy\")\n",
    "    Y_actinet = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/Y.npy\")\n",
    "    T_actinet = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/T.npy\")\n",
    "    P_actinet = np.load(f\"{SAVEFOLDER}/prepared/downsampling_linear_lowpass_None/pid.npy\")\n",
    "\n",
    "else:\n",
    "    X_actinet, Y_actinet, T_actinet, P_actinet = load_all_and_make_windows(\n",
    "        datafiles=glob(DATAFILES), \n",
    "        annofile=ANNOFILE, \n",
    "        out_dir=SAVEFOLDER, \n",
    "        anno_label=\"Walmsley2020\",\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        winsec=WINSEC,\n",
    "        n_jobs=N_JOBS,\n",
    "        downsampling_method=\"linear\",\n",
    "        lowpass_hz=None,\n",
    "        resample_rate=RESAMPLE_RATE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model using 5 fold stratified group cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbaa_classifier = BalancedRandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    oob_score=True,\n",
    "    sampling_strategy=\"not minority\",\n",
    "    replacement=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "actinet_classifier = ActivityClassifier(\n",
    "    labels = np.unique(Y_actinet),\n",
    "    batch_size=1000,\n",
    "    device=\"cuda:0\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate_models(\n",
    "    actinet_classifier,\n",
    "    bbaa_classifier,\n",
    "    X_actinet,\n",
    "    X_bbaa,\n",
    "    Y_actinet,\n",
    "    Y_bbaa,\n",
    "    P_actinet,\n",
    "    P_bbaa,\n",
    "    T_actinet,\n",
    "    T_bbaa,\n",
    "    weights_path=\"models/evaluation_models/actinet_vs_bbaa_{}.pt\",\n",
    "    out_dir=\"outputs/actinet_vs_bbaa\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bbaa = pd.read_pickle(\"outputs/actinet_vs_bbaa/rf_results.pkl\")\n",
    "results_actinet = pd.read_pickle(\"outputs/actinet_vs_bbaa/actinet_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'accelerometer': {'y': np.hstack(results_bbaa[\"Y_true\"]), \n",
    "                      'y_pred': np.hstack(results_bbaa[\"Y_pred\"]), \n",
    "                      'pid': np.hstack(results_bbaa[\"group\"])\n",
    "                      },\n",
    "    'actinet': {'y': np.hstack(results_actinet[\"Y_true\"]), \n",
    "                'y_pred': np.hstack(results_actinet[\"Y_pred\"]), \n",
    "                'pid': np.hstack(results_actinet[\"group\"])\n",
    "                }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    return accuracy, f1, kappa\n",
    "\n",
    "results = []\n",
    "\n",
    "for model, model_data in tqdm(data.items()):\n",
    "    for pid in np.unique(model_data['pid']):\n",
    "        mask = model_data['pid'] == pid\n",
    "        y_true = model_data['y'][mask]\n",
    "        y_pred = model_data['y_pred'][mask]\n",
    "        accuracy, f1, kappa = calculate_metrics(y_true, y_pred)\n",
    "        results.append({'Participant': pid, 'Model': model, \n",
    "                        'Accuracy': accuracy, 'Macro F1': f1, 'Cohen Kappa': kappa})\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model and calculate median, Q1, and Q3\n",
    "summary = results.groupby('Model')[['Accuracy', \n",
    "                                    'Macro F1', \n",
    "                                    'Cohen Kappa']].agg(lambda x: f\"{np.median(x):.3f} \" + \n",
    "                                                                  f\"({np.quantile(x, .25):.3f}, \" + \n",
    "                                                                  f\"{np.quantile(x, .75):.3f})\")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model):   \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')  # Normalized confusion matrix\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.3f', cbar=False, annot_kws={\"size\": 16})\n",
    "    plt.title(f'Confusion Matrix - {model}', fontsize=20)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    plt.xticks(ticks=np.arange(len(cm))+0.5, labels=np.unique(y_true), fontsize=14)\n",
    "    plt.yticks(ticks=np.arange(len(cm))+0.5, labels=np.unique(y_true), fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(data['accelerometer']['y'], data['accelerometer']['y_pred'], 'accelerometer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(data['actinet']['y'], data['actinet']['y_pred'], 'actinet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(metric='Macro F1', modulus=0):\n",
    "    # Create a boxplot overlay without outliers\n",
    "    sns.boxplot(x=\"Model\", y=metric, hue=\"Model\", data=results, width=0.3, showfliers=False)\n",
    "\n",
    "    # Create a stripplot for the jittered points\n",
    "    sns.stripplot(x=\"Model\", y=metric, data=results, \n",
    "                  jitter=True, color='black', alpha=0.3)\n",
    "\n",
    "    # Draw lines between points for each participant\n",
    "    i = 0\n",
    "    for pid in results['Participant'].unique():\n",
    "        if modulus and i%modulus == 0:\n",
    "            pid_df = results[results['Participant'] == pid]\n",
    "            plt.plot(pid_df['Model'], pid_df[metric], marker='', \n",
    "                     linestyle='-', color='grey', alpha=0.3)\n",
    "        i += 1\n",
    "\n",
    "    plt.title(f\"Comparison of {metric} between\\naccelerometer and actinet models\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance('Macro F1', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance('Cohen Kappa', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "actinet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
